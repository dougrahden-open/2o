# maurogpt2-model.py - Main API Server with BGE Search + OpenChat Analysis

# %% Cell 1: Imports and Dependencies

import os
import asyncio
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from contextlib import asynccontextmanager
import json
import sqlite3
import re

import uvicorn
from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel

import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# %% Cell 2: Configuration

BASE_DIR = Path(os.environ.get("MAUROGPT2_BASE", "D:/aa039v2"))
MODELS_DIR = BASE_DIR / "models"
ENGINE_DIR = BASE_DIR / "Engine"
TEMPLATES_DIR = BASE_DIR / "templates"
STATIC_DIR = BASE_DIR / "static"

# Model paths
BGE_MODEL_PATH = MODELS_DIR / "bge-large-en-v15"
OPENCHAT_MODEL_PATH = MODELS_DIR / "openchat-35-0106"
COLLECTION_NAME = "pdf_collection"

# Search settings
DEFAULT_TOP_K = 20
DEFAULT_SIMILARITY_CUTOFF = 0.2
MAX_CONTEXT_TOKENS = 4000

# Server settings
HOST = "0.0.0.0"
PORT = 8000
DEBUG = True

# Global model storage
models = {}

# %% Cell 3: Enhanced Data Models

class SearchRequest(BaseModel):
    question: str
    top_k: Optional[int] = DEFAULT_TOP_K
    similarity_cutoff: Optional[float] = DEFAULT_SIMILARITY_CUTOFF
    include_analysis: Optional[bool] = True

class SearchResult(BaseModel):
    document: str
    source_pdf: str
    page_number: int
    similarity_score: float
    chunk_index: int

class AnalysisResult(BaseModel):
    summary: str
    confidence_probability: float
    suggested_better_prompt: str
    key_source_pdfs: List[str]

class SearchResponse(BaseModel):
    question: str
    results: List[SearchResult]
    context_used: str
    analysis: Optional[AnalysisResult] = None
    processing_time: float
    total_results: int

class HealthResponse(BaseModel):
    status: str
    models_loaded: Dict[str, bool]
    database_status: str
    timestamp: str

# %% Cell 4: Model Loading Functions

async def load_bge_model():
    """Load BGE embedding model"""
    print("Loading BGE embedding model...")

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is required but not available for BGE model")

    try:
        model = SentenceTransformer(str(BGE_MODEL_PATH), device='cuda')
        print(f"BGE model loaded on device: {model.device}")
        return model
    except Exception as e:
        print(f"Error loading BGE model: {e}")
        return None

async def load_openchat_model():
    """Load OpenChat model and tokenizer"""
    print("Loading OpenChat model...")

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is required but not available for OpenChat model")

    try:
        tokenizer = AutoTokenizer.from_pretrained(str(OPENCHAT_MODEL_PATH))
        model = AutoModelForCausalLM.from_pretrained(
            str(OPENCHAT_MODEL_PATH),
            torch_dtype=torch.float16,
            device_map={"": "cuda"},  # Force full model to single GPU
            trust_remote_code=True
        )
        print("OpenChat model loaded")
        return {"model": model, "tokenizer": tokenizer}

    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            raise RuntimeError("CUDA out of memory while loading OpenChat model. Try reducing model size or increasing GPU memory.")
        raise

    except Exception as e:
        print(f"Error loading OpenChat model: {e}")
        return None

async def load_chromadb():
    """Connect to ChromaDB"""
    print("Connecting to ChromaDB...")
    try:
        client = chromadb.PersistentClient(path=str(ENGINE_DIR))
        collection = client.get_collection(COLLECTION_NAME)
        count = collection.count()
        print(f"ChromaDB connected: {count:,} embeddings available")
        return collection
    except Exception as e:
        print(f"Error connecting to ChromaDB: {e}")
        return None

async def load_all_models():
    """Load all models concurrently"""
    print("Starting concurrent model load...")

    bge_task = load_bge_model()
    openchat_task = load_openchat_model()
    chromadb_task = load_chromadb()

    bge_model, openchat, chromadb = await asyncio.gather(
        bge_task, openchat_task, chromadb_task
    )

    models["bge"] = bge_model
    models["openchat"] = openchat
    models["chromadb"] = chromadb

    loaded = sum(1 for m in [bge_model, openchat, chromadb] if m is not None)
    print(f"Loaded {loaded}/3 models successfully")

# %% Cell 5: Vector Search Functions

def search_documents(
    question: str,
    top_k: int = DEFAULT_TOP_K,
    similarity_cutoff: float = DEFAULT_SIMILARITY_CUTOFF
) -> Dict:
    """Search documents using BGE embeddings"""
    start_time = datetime.now()

    # Get models
    embedding_model = models.get("bge")
    collection = models.get("chromadb")

    if not embedding_model or not collection:
        raise HTTPException(status_code=500, detail="Models not loaded")

    # Generate query embedding
    query_embedding = embedding_model.encode([question]).tolist()

    # Search ChromaDB
    results = collection.query(
        query_embeddings=query_embedding,
        n_results=min(top_k * 10, 500),  # Over-fetch to allow filtering
        include=["documents", "metadatas", "distances"]
    )

    # Extract results
    documents = results["documents"][0]
    metadatas = results["metadatas"][0]
    distances = results["distances"][0]

    search_results = []
    for doc, meta, dist in zip(documents, metadatas, distances):
        similarity = 1 - dist  # Convert distance to similarity

        if similarity >= similarity_cutoff:
            search_results.append({
                "document": doc,
                "source_pdf": meta.get("source_pdf", "unknown"),
                "page_number": meta.get("page_number", 0),
                "chunk_index": meta.get("chunk_index", 0),
                "similarity_score": similarity,
                "word_count": meta.get("word_count", 0)
            })

    # Sort by similarity and trim to top_k
    search_results.sort(key=lambda x: x["similarity_score"], reverse=True)
    search_results = search_results[:top_k]

    # Build context string with token budgeting
    context_lines = []
    tokens_used = 0

    for result in search_results:
        filename = result["source_pdf"]
        page = result["page_number"]
        similarity = result["similarity_score"]

        line = f"[{filename} | page {page} | similarity {similarity:.4f}]\n{result['document'].strip()}"
        estimated_tokens = len(line.split()) * 1.3  # Conservative estimate

        if tokens_used + estimated_tokens > MAX_CONTEXT_TOKENS:
            break

        context_lines.append(line)
        tokens_used += estimated_tokens

    context_text = "\n\n".join(context_lines)
    processing_time = (datetime.now() - start_time).total_seconds()

    return {
        "results": search_results,
        "context": context_text,
        "processing_time": processing_time,
        "total_results": len(search_results)
    }

# %% Cell 6: Enhanced OpenChat Analysis Functions

def parse_structured_response(response_text: str) -> AnalysisResult:
    """Parse the structured response from OpenChat"""
    summary = "Analysis could not be parsed."
    confidence = 0.5
    better_prompt = "Try a more specific technical question."
    key_pdfs = []

    try:
        summary_match = re.search(
            r'SUMMARY:\s*(.*?)(?=\n(?:CONFIDENCE|$))',
            response_text, re.DOTALL | re.IGNORECASE
        )
        if summary_match:
            summary = summary_match.group(1).strip()[:500]

        confidence_match = re.search(
            r'CONFIDENCE:\s*([\d.]+)',
            response_text, re.IGNORECASE
        )
        if confidence_match:
            try:
                confidence = float(confidence_match.group(1))
                if confidence > 1.0:
                    confidence = confidence / 100.0
                confidence = max(0.0, min(1.0, confidence))
            except ValueError:
                confidence = 0.5

        prompt_match = re.search(
            r'BETTER_PROMPT:\s*(.*?)(?=\n(?:KEY_PDFS|$))',
            response_text, re.DOTALL | re.IGNORECASE
        )
        if prompt_match:
            better_prompt = prompt_match.group(1).strip()[:300]

        pdfs_match = re.search(
            r'KEY_PDFS:\s*(.*?)$',
            response_text, re.DOTALL | re.IGNORECASE
        )
        if pdfs_match:
            pdfs_text = pdfs_match.group(1).strip()
            pdf_matches = re.findall(r'([^\s,]+\.pdf)', pdfs_text, re.IGNORECASE)
            key_pdfs = list(set(pdf_matches))[:5]

    except Exception as e:
        print(f"Error parsing structured response: {e}")
        summary = "Error parsing analysis response."
        confidence = 0.3
        better_prompt = "Try rephrasing your question with more specific technical terms."
        key_pdfs = []

    return AnalysisResult(
        summary=summary,
        confidence_probability=confidence,
        suggested_better_prompt=better_prompt,
        key_source_pdfs=key_pdfs
    )

def analyze_with_openchat_structured(
    question: str,
    context: str,
    search_results: List[Dict]
) -> AnalysisResult:
    """Generate structured analysis using OpenChat model"""
    openchat = models.get("openchat")
    if not openchat:
        return AnalysisResult(
            summary="OpenChat model not available.",
            confidence_probability=0.0,
            suggested_better_prompt="Please ensure OpenChat model is loaded.",
            key_source_pdfs=[]
        )

    model = openchat["model"]
    tokenizer = openchat["tokenizer"]

    source_pdfs = list(set(result["source_pdf"] for result in search_results))

    prompt = (
        f"You are analyzing technical document search results. Provide a structured response with exactly these 4 sections:\n\n"
        f"Question: {question}\n\n"
        f"Available PDFs: {', '.join(source_pdfs)}\n\n"
        f"Context:\n{context}\n\n"
        f"Provide your analysis in exactly this format:\n\n"
        f"SUMMARY: [Provide a concise 100-word or less summary of the answer based on the context]\n\n"
        f"CONFIDENCE: [Provide a probability from 0.0 to 1.0 of how confident you are in this answer]\n\n"
        f"BETTER_PROMPT: [Suggest a more specific prompt that would get better vector search results]\n\n"
        f"KEY_PDFS: [List the 2-3 most relevant PDF filenames that were used to answer this question]\n\n"
        f"Response:"
    )

    try:
        inputs = tokenizer.encode(
            prompt,
            return_tensors="pt",
            max_length=3500,
            truncation=True
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=400,
                temperature=0.1,
                do_sample=False,
                num_beams=1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        prompt_trimmed = tokenizer.decode(inputs[0], skip_special_tokens=True).strip()
        analysis_text = decoded_output[len(prompt_trimmed):].strip()

        return parse_structured_response(analysis_text)

    except Exception as e:
        print(f"Error generating structured analysis: {str(e)}")
        return AnalysisResult(
            summary=f"Error generating analysis: {str(e)}",
            confidence_probability=0.2,
            suggested_better_prompt="Try a simpler, more direct question about specific technical aspects.",
            key_source_pdfs=source_pdfs[:3] if source_pdfs else []
        )

# %% Cell 7: API Endpoints

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Load models on startup"""
    print("Starting MauroGPT2 Model Server...")
    await load_all_models()
    yield
    print("Shutting down MauroGPT2 Model Server...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# Create FastAPI app
app = FastAPI(
    title="MauroGPT2 Model Server",
    description="BGE Vector Search + Structured OpenChat Analysis for Technical Documents",
    version="2.1.0",
    lifespan=lifespan
)

# Health check endpoint
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    models_status = {
        "bge": models.get("bge") is not None,
        "openchat": models.get("openchat") is not None,
        "chromadb": models.get("chromadb") is not None
    }
    db_status = "connected" if models.get("chromadb") else "disconnected"

    return HealthResponse(
        status="healthy" if all(models_status.values()) else "degraded",
        models_loaded=models_status,
        database_status=db_status,
        timestamp=datetime.now().isoformat()
    )

# Main search endpoint with structured analysis
@app.post("/api/search", response_model=SearchResponse)
async def search_endpoint(request: SearchRequest):
    """Main search endpoint with structured analysis"""
    try:
        search_data = search_documents(
            question=request.question,
            top_k=request.top_k,
            similarity_cutoff=request.similarity_cutoff
        )

        search_results = [
            SearchResult(**result) for result in search_data["results"]
        ]

        analysis = None
        if request.include_analysis and search_data["context"]:
            analysis = analyze_with_openchat_structured(
                request.question,
                search_data["context"],
                search_data["results"]
            )

        return SearchResponse(
            question=request.question,
            results=search_results,
            context_used=search_data["context"],
            analysis=analysis,
            processing_time=search_data["processing_time"],
            total_results=search_data["total_results"]
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# %% Cell 8: Main Server Function

def main():
    """Main server function"""
    print("=" * 60)
    print("MAUROGPT2 MODEL SERVER - ENHANCED")
    print("=" * 60)
    print(f"Base Directory: {BASE_DIR}")
    print(f"BGE Model: {BGE_MODEL_PATH}")
    print(f"OpenChat Model: {OPENCHAT_MODEL_PATH}")
    print(f"Server: http://{HOST}:{PORT}")
    print("=" * 60)

    uvicorn.run(
        "maurogpt2-model:app",
        host=HOST,
        port=PORT,
        reload=DEBUG,
        log_level="info"
    )

if __name__ == "__main__":
    main()
